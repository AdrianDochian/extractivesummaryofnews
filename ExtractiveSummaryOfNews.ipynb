{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bd338fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import functools\n",
    "import operator\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2207c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/samanu/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eceb5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress_bar(current_value, max_value, bar_length = 100):\n",
    "    # compute bar properties\n",
    "    ratio = current_value / max_value\n",
    "    current_bar_length = int(ratio * bar_length)\n",
    "    \n",
    "    # compute bar\n",
    "    current_bar_array = [' '] * bar_length\n",
    "    for i in range(0, current_bar_length):\n",
    "        current_bar_array[i] = '#'\n",
    "    bar_string = functools.reduce(operator.add, current_bar_array)\n",
    "    \n",
    "    # print bar\n",
    "    clear_output(wait = True)\n",
    "    print(\"[{}] {}%\".format(bar_string, int(ratio * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74e2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_PATH = 'BBC News Summary/News Articles'\n",
    "SUMMARIES_PATH = 'BBC News Summary/Summaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcbc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE='article'\n",
    "SUMMARY='summary'\n",
    "ENCODING = 'latin1'\n",
    "\n",
    "\n",
    "CLASSES = ['sport', 'entertainment', 'tech', 'politics', 'business']\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "\n",
    "data_set = dict()\n",
    "\n",
    "# DICTIONARY INDEXES\n",
    "FILE_NAME = 0\n",
    "TITLE_WORDS = 1\n",
    "TITLE_UNIGRAMS = 2\n",
    "TITLE_BIGRAMS = 3\n",
    "TITLE_FOURGRAMS = 4\n",
    "FILE_SENTENCES = 5\n",
    "FILE_UNIGRAMS = 6\n",
    "FILE_BIGRAMS = 7\n",
    "FILE_FOURGRAMS = 8\n",
    "SUMMARY_TEXT = 9\n",
    "SUMMARY_UNIGRAMS = 10\n",
    "SUMMARY_BIGRAMS = 10\n",
    "SUMMARY_FOURGRAMS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb81bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_words(initial_words):\n",
    "    return list(filter(lambda x: lemmatizer.lemmatize(x), initial_words))\n",
    "\n",
    "def filter_stop_words(initial_words):\n",
    "    return list(filter(lambda x : x not in stop_words, initial_words))\n",
    "\n",
    "def only_nouns(word_list):\n",
    "    return [word for (word, tag) in pos_tag(word_list) if tag[:2] == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c4a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_text_for_article(class_name, file_name):\n",
    "    summary_directory_path = os.path.join(SUMMARIES_PATH, class_name)\n",
    "    summary_file_path = os.path.join(summary_directory_path, file_name)\n",
    "    with open (summary_file_path, 'r', encoding = ENCODING) as file:\n",
    "        return file.read()\n",
    "\n",
    "def n_grams(text, n):\n",
    "    return list(ngrams(text, n))\n",
    "    \n",
    "def prepare_sentence_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    without_stop_words = filter_stop_words(words)\n",
    "    nouns = only_nouns(without_stop_words)\n",
    "    lemmatized = lemmatize_words(nouns)\n",
    "    return lemmatized\n",
    "    \n",
    "def persist_article_class_file(class_name, file_name, file_path):\n",
    "\n",
    "    with open (file_path, 'r', encoding = 'latin1') as file:\n",
    "        # Title\n",
    "        file_title = file.readline()\n",
    "\n",
    "        title_words = prepare_sentence_words(file_title)\n",
    "        title_unigrams = n_grams(title_words, 1)\n",
    "        title_bigrams = n_grams(title_words, 2)\n",
    "        title_fourgrams = n_grams(title_words, 4)\n",
    "        \n",
    "        file.readline()\n",
    "\n",
    "        # Content\n",
    "        file_content = file.read()\n",
    "        \n",
    "        file_sentences = list(map(lambda sentence : prepare_sentence_words(sentence), sent_tokenize(file_content)))\n",
    "        file_unigrams = list(map(lambda sentence : n_grams(sentence, 1), file_sentences))\n",
    "        file_bigrams = list(map(lambda sentence : n_grams(sentence, 2), file_sentences))\n",
    "        file_fourgrams = list(map(lambda sentence : n_grams(sentence, 4), file_sentences))\n",
    "        \n",
    "        # Summary\n",
    "        summary_text = get_summary_text_for_article(class_name, file_name)\n",
    "        summary_tokens = word_tokenize(summary_text)\n",
    "        summary_unigrams = n_grams(summary_tokens, 1)\n",
    "        summary_bigrams = n_grams(summary_tokens, 2)\n",
    "        summary_fourgrams = n_grams(summary_tokens, 4)\n",
    "        \n",
    "        \n",
    "        # Wrapper\n",
    "        new_information = (file_name, \\\n",
    "                           title_words, \\\n",
    "                           title_unigrams, \\\n",
    "                           title_bigrams, \\\n",
    "                           title_fourgrams, \\\n",
    "                           sent_tokenize(file_content), \\\n",
    "                           file_unigrams, \\\n",
    "                           file_bigrams, \\\n",
    "                           file_fourgrams, \\\n",
    "                           summary_text, \\\n",
    "                           summary_unigrams, \\\n",
    "                           summary_bigrams, \\\n",
    "                           summary_fourgrams)\n",
    "    \n",
    "        if class_name in data_set:\n",
    "            new_value = data_set[class_name]\n",
    "            new_value.append(new_information)\n",
    "            data_set[class_name] = new_value\n",
    "        else:\n",
    "            data_set[class_name] = [new_information]\n",
    "\n",
    "def persist_article_class(class_name, class_directory_path):\n",
    "    for file_name in os.listdir(class_directory_path):\n",
    "        persist_article_class_file(class_name, file_name, os.path.join(class_directory_path, file_name))\n",
    "\n",
    "def persist_articles_information():\n",
    "    for class_directory in os.listdir(ARTICLES_PATH):\n",
    "        persist_article_class(class_directory, os.path.join(ARTICLES_PATH, class_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f566271",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_articles_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f22daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_summary():\n",
    "    train_data = dict()\n",
    "    test_data = dict()\n",
    "    \n",
    "    for class_name in data_set:\n",
    "        data_array = data_set[class_name]\n",
    "        total_data_length = len(data_array)\n",
    "        \n",
    "        cutoff_index = int(total_data_length * TRAIN_PERCENTAGE)\n",
    "        \n",
    "        train_data[class_name] = data_array[0: cutoff_index]\n",
    "        test_data[class_name] = data_array[cutoff_index:total_data_length]    \n",
    "        \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e656be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = prepare_data_for_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e06f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_term_frequency(item, items_matrix):\n",
    "    count = 0\n",
    "    for i in range(0, len(items_matrix)):\n",
    "        for j in range(0, len(items_matrix[i])):\n",
    "            if items_matrix[i][j] == item:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_inverse_document_frequency(item, class_data, content_index):\n",
    "    count = 0\n",
    "    # for each document\n",
    "    for article_information in class_data:\n",
    "        items_matrix = article_information[content_index]\n",
    "        # for each sentence\n",
    "        for i in range(0, len(items_matrix)):\n",
    "            \n",
    "            # if found in any sentence => found in article \n",
    "            if item in items_matrix[i]:\n",
    "                count+=1\n",
    "                continue\n",
    "\n",
    "    return count\n",
    "\n",
    "def compute_tf_idf(content_index, title_index):\n",
    "    N = 0\n",
    "    tf_idf_model = dict()\n",
    "    for class_name in CLASSES:\n",
    "        tf_idf_model[class_name] = dict()\n",
    "        N += len(train_data[class_name])\n",
    "\n",
    "    index = 1\n",
    "    for class_name in train_data:\n",
    "        class_data = train_data[class_name]\n",
    "        \n",
    "        for article_information in class_data:\n",
    "            show_progress_bar(index, N)\n",
    "            n_gram_matrix = article_information[content_index]\n",
    "            for i in range(0, len(n_gram_matrix)):\n",
    "                for j in range(0, len(n_gram_matrix[i])):\n",
    "                    n_gram = n_gram_matrix[i][j]\n",
    "                    term_frequency = count_term_frequency(n_gram, article_information[content_index])\n",
    "                    inverse_document_frequency = count_inverse_document_frequency(n_gram, class_data, content_index)\n",
    "\n",
    "                    tf_idf = term_frequency * math.log(N / inverse_document_frequency)\n",
    "                    \n",
    "                    if n_gram in tf_idf_model[class_name]:\n",
    "                        new_value = tf_idf_model[class_name][n_gram]\n",
    "                        new_value += tf_idf\n",
    "                        tf_idf_model[class_name][n_gram] = new_value\n",
    "                    else:\n",
    "                        tf_idf_model[class_name][n_gram] = tf_idf\n",
    "                    \n",
    "            index += 1\n",
    "    \n",
    "    return tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0af1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "tf_idf_unigram_model = compute_tf_idf(FILE_UNIGRAMS, TITLE_UNIGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee630920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "tf_idf_bigram_model = compute_tf_idf(FILE_BIGRAMS, TITLE_BIGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4145b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################################################################################] 100%\n"
     ]
    }
   ],
   "source": [
    "tf_idf_fourgram_model = compute_tf_idf(FILE_FOURGRAMS, TITLE_FOURGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4352652",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_IMPORTANCE = 0.1\n",
    "K = 3\n",
    "\n",
    "def generate_sentence_smoother_sequence(number_of_sentences):\n",
    "    SENTENCE_ORDER_LOWER_BOUND = 0.1\n",
    "    SENTENCE_ORDER_UPPER_BOUND = 1\n",
    "    return np.linspace(SENTENCE_ORDER_LOWER_BOUND, SENTENCE_ORDER_UPPER_BOUND, number_of_sentences)\n",
    "\n",
    "def count_of_words_in_title(n_gram_sentence, title_n_grams):\n",
    "    count = 0\n",
    "    for n_gram in n_gram_sentence:\n",
    "        if n_gram in title_n_grams:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def produce_summary_for_class(model, class_name, article_information, file_n_grams_index, title_n_grams_index):\n",
    "    sentences = []\n",
    "    sentences_length = len(article_information[FILE_SENTENCES])\n",
    "    for i in range(0, sentences_length):\n",
    "        actual_sentence = article_information[FILE_SENTENCES][i]\n",
    "        n_gram_sentence = article_information[file_n_grams_index][i]\n",
    "        \n",
    "        sentence_score = 0\n",
    "        \n",
    "        # step 1 (Importance of nouns)\n",
    "        nouns_score = 0\n",
    "        for n_gram in n_gram_sentence:\n",
    "            if n_gram in model[class_name]:\n",
    "                nouns_score += model[class_name][n_gram]\n",
    "        \n",
    "        sentence_score = nouns_score / (len(n_gram_sentence) + 1)\n",
    "        \n",
    "        # step 2 (Relation with title)\n",
    "        words_in_title = count_of_words_in_title(n_gram_sentence, article_information[title_n_grams_index])\n",
    "        title_score = words_in_title * TITLE_IMPORTANCE\n",
    "        \n",
    "        sentence_score += title_score\n",
    "        \n",
    "        sentences.append((actual_sentence, sentence_score))\n",
    "    \n",
    "    # step 3 (Smoothing sentences with respect to their position in article)\n",
    "    sentence_smoother = generate_sentence_smoother_sequence(sentences_length)\n",
    "    for i in range(0, sentences_length):\n",
    "        text, score = sentences[i] \n",
    "        new_score = score * sentence_smoother[i]\n",
    "        sentences[i] = (text, new_score) \n",
    "    \n",
    "    # step 4 (Fetch only the first K sentences with highest score)\n",
    "    sentences.sort(reverse = True, key = lambda x : x[1])\n",
    "    \n",
    "    # chose upper bound\n",
    "    upper_bound = K if K <= sentences_length else sentences_length \n",
    "    \n",
    "    # filter out the score\n",
    "    sentences_test_list = list(map(lambda x: x[0], sentences[0:upper_bound]))\n",
    "    \n",
    "    # concat sentences\n",
    "    return functools.reduce(operator.add, sentences_test_list)\n",
    "    \n",
    "def produce_summary(model, file_n_grams_index, title_n_grams_index):\n",
    "    results = []\n",
    "    for class_name in test_data:\n",
    "        class_data = test_data[class_name]\n",
    "        \n",
    "        for article_information in class_data:\n",
    "            computed_summary = produce_summary_for_class(\\\n",
    "                model, class_name, article_information, file_n_grams_index, title_n_grams_index)\n",
    "            results.append((class_name, article_information, computed_summary))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_results = produce_summary(tf_idf_unigram_model, FILE_UNIGRAMS, TITLE_UNIGRAMS)\n",
    "bigrams_results = produce_summary(tf_idf_bigram_model, FILE_BIGRAMS, TITLE_FOURGRAMS)\n",
    "fourgrams_results = produce_summary(tf_idf_bigram_model, FILE_BIGRAMS, TITLE_FOURGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1f1b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "\n",
    "def n_grams(text, n):\n",
    "    return list(ngrams(text.split(), n))\n",
    "\n",
    "def common(list1, list2): \n",
    "    return set(list1).intersection(list2)\n",
    "\n",
    "def bleu_n(results, n_gram_identifier):\n",
    "    total_common_grams = 0\n",
    "    total_text_grams = 0\n",
    "    for result in results:\n",
    "        expected = n_grams(result[1][SUMMARY_TEXT], n_gram_identifier)\n",
    "        actual = n_grams(result[2], n_gram_identifier)\n",
    "        common_grams = common(expected, actual)\n",
    "        total_common_grams += len(common_grams) \n",
    "        total_text_grams += len(actual)\n",
    "        \n",
    "    return total_common_grams / total_text_grams * 100\n",
    "\n",
    "def rouge_n(results, n_gram_identifier):\n",
    "    total_common_grams = 0\n",
    "    total_text_grams = 0\n",
    "    for result in results:\n",
    "        expected = n_grams(result[1][SUMMARY_TEXT], n_gram_identifier)\n",
    "        actual = n_grams(result[2], n_gram_identifier)\n",
    "        common_grams = common(expected, actual)\n",
    "        total_common_grams += len(common_grams) \n",
    "        total_text_grams += len(expected)\n",
    "        \n",
    "    return total_common_grams / total_text_grams * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c65a0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['UNIGRAMS', 'BIGRAMS', 'FOURGRAMS']\n",
    "precision = [bleu_n(unigrams_results, 1), bleu_n(bigrams_results, 2), bleu_n(fourgrams_results, 4)]\n",
    "recall = [rouge_n(unigrams_results, 1), rouge_n(unigrams_results, 1), rouge_n(unigrams_results, 1)]\n",
    "\n",
    "# label locations\n",
    "x = np.arange(len(labels))  \n",
    "# bars width\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, precision, width, label='Precision')\n",
    "rects2 = ax.bar(x + width/2, recall, width, label='Recall')\n",
    "\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('TF-IDF results based on n-grams')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=1)\n",
    "ax.bar_label(rects2, padding=1)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
